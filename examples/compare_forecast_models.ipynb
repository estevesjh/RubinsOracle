{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# EnsembleForecaster Exploration\n",
    "\n",
    "This notebook demonstrates the EnsembleForecaster class which combines multiple Prophet and/or NeuralProphet models for frequency-decomposed forecasting.\n",
    "\n",
    "**Strategies compared:**\n",
    "1. **Baseline Prophet** - Single Prophet on raw signal\n",
    "2. **Dual Prophet (sum)** - High-freq + Low-freq Prophet with bias correction\n",
    "3. **Dual Prophet (reconcile)** - Learned weights from in-sample predictions\n",
    "4. **Dual NeuralProphet** - Same setup with NeuralProphet models\n",
    "5. **NeuralProphet with lagged regressors** - Using decomposed bands as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/esteves/Documents/github/RubinsOracle\n",
      "Data path: /Users/esteves/Documents/github/RubinsOracle/data/temp2024.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from rubin_oracle import (\n",
    "    BandpassDecomposer,\n",
    "    EnsembleConfig,\n",
    "    EnsembleForecaster,\n",
    "    ProphetConfig,\n",
    "    ProphetForecaster,\n",
    ")\n",
    "\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"temp2024.csv\"\n",
    "CONFIGS_PATH = PROJECT_ROOT / \"configs\"\n",
    "\n",
    "# Constants\n",
    "STEPS_PER_HOUR = 4\n",
    "STEPS_PER_DAY = 96\n",
    "\n",
    "# Styling\n",
    "sns.set_context(\"notebook\", font_scale=0.9)\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "COLORS = {\n",
    "    \"actual\": \"#2c3e50\",\n",
    "    \"baseline\": \"#95a5a6\",\n",
    "    \"dual_sum\": \"#3498db\",\n",
    "    \"dual_reconcile\": \"#9b59b6\",\n",
    "    \"dual_np\": \"#e74c3c\",\n",
    "    \"np_lagged\": \"#27ae60\",\n",
    "}\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 35,132 samples (366.0 days)\n",
      "Date range: 2024-01-01 00:15:00 to 2025-01-01 00:00:00\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:15:00</td>\n",
       "      <td>12.853100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:30:00</td>\n",
       "      <td>12.764791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:45:00</td>\n",
       "      <td>12.777040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "      <td>12.777202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 01:15:00</td>\n",
       "      <td>12.751278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ds          y\n",
       "0 2024-01-01 00:15:00  12.853100\n",
       "1 2024-01-01 00:30:00  12.764791\n",
       "2 2024-01-01 00:45:00  12.777040\n",
       "3 2024-01-01 01:00:00  12.777202\n",
       "4 2024-01-01 01:15:00  12.751278"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load temperature data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"ds\"] = pd.to_datetime(df[\"ds\"], utc=True).dt.tz_convert(\"America/Santiago\")\n",
    "df = df[[\"ds\", \"y\"]].copy()\n",
    "\n",
    "# Remove timezone for Prophet compatibility\n",
    "df[\"ds\"] = df[\"ds\"].dt.tz_localize(None)\n",
    "\n",
    "# Drop duplicates AFTER removing timezone (DST transitions can create duplicates)\n",
    "df = df.drop_duplicates(subset=\"ds\", keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# # Drop NaN values\n",
    "# n_nan = df['y'].isna().sum()\n",
    "# if n_nan > 0:\n",
    "#     print(f\"Dropping {n_nan} NaN values\")\n",
    "#     df = df.dropna(subset=['y']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset: {len(df):,} samples ({len(df) / STEPS_PER_DAY:.1f} days)\")\n",
    "print(f\"Date range: {df['ds'].min()} to {df['ds'].max()}\")\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 29,952 samples (312 days)\n",
      "Test: 96 samples (24 hours)\n",
      "\n",
      "Forecast start: 2024-11-08 01:15:00\n"
     ]
    }
   ],
   "source": [
    "# Use 300 days for training, forecast 24h ahead\n",
    "TRAIN_DAYS = 300\n",
    "FORECAST_HORIZON = 96  # 24 hours\n",
    "\n",
    "train_samples = TRAIN_DAYS * STEPS_PER_DAY + 12 * STEPS_PER_DAY\n",
    "df_train = df.iloc[:train_samples].copy()\n",
    "df_test = df.iloc[train_samples : train_samples + FORECAST_HORIZON].copy()\n",
    "\n",
    "print(f\"Training: {len(df_train):,} samples ({len(df_train) / STEPS_PER_DAY:.0f} days)\")\n",
    "print(f\"Test: {len(df_test)} samples ({len(df_test) / STEPS_PER_HOUR:.0f} hours)\")\n",
    "print(f\"\\nForecast start: {df_test['ds'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Baseline Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit baseline Prophet (single model on raw signal)\n",
    "print(\"Fitting Baseline Prophet...\")\n",
    "\n",
    "baseline_config = ProphetConfig(\n",
    "    lag_days=7,\n",
    "    n_forecast=FORECAST_HORIZON,\n",
    "    freq=\"15min\",\n",
    "    daily_seasonality=13,\n",
    "    weekly_seasonality=8,\n",
    "    yearly_seasonality=False,\n",
    "    n_changepoints=24,\n",
    "    changepoint_prior_scale=0.05,\n",
    ")\n",
    "\n",
    "baseline_model = ProphetForecaster(baseline_config)\n",
    "baseline_model.fit(df_train)\n",
    "\n",
    "# Show in-sample metrics\n",
    "print(\"\\nBaseline Prophet Metrics:\")\n",
    "for key, val in baseline_model.metrics_.items():\n",
    "    print(f\"  {key}: {val:.4f}\" if isinstance(val, float) else f\"  {key}: {val}\")\n",
    "\n",
    "# Predict\n",
    "baseline_fc = baseline_model.predict(periods=FORECAST_HORIZON)\n",
    "yhat_baseline = baseline_fc[\"yhat\"].values\n",
    "\n",
    "# Apply bias correction\n",
    "bias = df_train[\"y\"].iloc[-1] - yhat_baseline[0]\n",
    "yhat_baseline_bias = yhat_baseline + bias\n",
    "\n",
    "y_true = df_test[\"y\"].values\n",
    "rmse_baseline = np.sqrt(((y_true - yhat_baseline_bias) ** 2).mean())\n",
    "print(f\"\\nBaseline RMSE (with bias correction): {rmse_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline Prophet using the new plot() method\n",
    "fig, ax = baseline_model.plot(\n",
    "    forecast_df=baseline_fc, window_days=7, title=\"Baseline Prophet - Fit and Forecast\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Dual Prophet Ensemble (sum + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and fit Dual Prophet with sum combination\n",
    "print(\"Fitting Dual Prophet Ensemble (sum + bias)...\")\n",
    "\n",
    "config_dual_sum = EnsembleConfig.from_yaml(CONFIGS_PATH / \"ensemble_dual_prophet.yaml\")\n",
    "print(f\"\\nConfig: {config_dual_sum.name}\")\n",
    "print(f\"Components: {[c.name for c in config_dual_sum.components]}\")\n",
    "print(f\"Combine method: {config_dual_sum.combine_method}\")\n",
    "print(f\"Bias correction: {config_dual_sum.post_processor.bias_correction}\")\n",
    "\n",
    "model_dual_sum = EnsembleForecaster(config_dual_sum)\n",
    "model_dual_sum.fit(df_train)\n",
    "\n",
    "# Display ensemble metrics\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ENSEMBLE METRICS (from model.metrics_)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall RMSE: {model_dual_sum.metrics_['rmse']:.4f}\")\n",
    "print(f\"Overall MAE:  {model_dual_sum.metrics_['mae']:.4f}\")\n",
    "print(f\"Overall R²:   {model_dual_sum.metrics_['r2']:.4f}\")\n",
    "print(f\"Samples:      {model_dual_sum.metrics_['n_samples']}\")\n",
    "print(\"\\nComponent metrics:\")\n",
    "for name, m in model_dual_sum.metrics_[\"components\"].items():\n",
    "    print(f\"  {name}: RMSE={m['rmse']:.4f}, MAE={m['mae']:.4f}, R²={m['r2']:.4f}\")\n",
    "\n",
    "# Predict\n",
    "fc_dual_sum = model_dual_sum.predict(df_train.tail(60 * STEPS_PER_DAY), periods=FORECAST_HORIZON)\n",
    "yhat_dual_sum = fc_dual_sum[\"yhat\"].values\n",
    "\n",
    "rmse_dual_sum = np.sqrt(((y_true - yhat_dual_sum) ** 2).mean())\n",
    "print(f\"\\nDual Prophet (sum) Forecast RMSE: {rmse_dual_sum:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8m0tqtq6zqb",
   "metadata": {},
   "source": [
    "### 4.1 Ensemble Plots\n",
    "\n",
    "Use `model.plot()` for combined ensemble view and `model.plot_components()` for per-component panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdz7astdlv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ensemble fit and forecast (combined view)\n",
    "fig, ax = model_dual_sum.plot(\n",
    "    df_train, window_days=7, title=\"Dual Prophet Ensemble - Combined Fit and Forecast\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0izpjrihmpcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual components (high_freq and low_freq)\n",
    "fig, axes = model_dual_sum.plot_components(df_train, window_days=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hg8324yargi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access individual component models\n",
    "print(\"Component model access:\")\n",
    "print(f\"  model.get_component_names() = {model_dual_sum.get_component_names()}\")\n",
    "print(f\"  model.components.keys() = {list(model_dual_sum.components.keys())}\")\n",
    "print(f\"  model.high_freq_model = {type(model_dual_sum.high_freq_model).__name__}\")\n",
    "print(f\"  model.low_freq_model = {type(model_dual_sum.low_freq_model).__name__}\")\n",
    "\n",
    "# Access high_freq model metrics\n",
    "print(\"\\nHigh-freq component metrics:\")\n",
    "hf_model = model_dual_sum.high_freq_model\n",
    "for key, val in hf_model.metrics_.items():\n",
    "    print(f\"  {key}: {val:.4f}\" if isinstance(val, float) else f\"  {key}: {val}\")\n",
    "\n",
    "# Plot individual component using Prophet's plot method\n",
    "print(\"\\nPlotting high_freq component:\")\n",
    "fig, ax = hf_model.plot(window_days=7, title=\"High-freq Component (Prophet)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## 5. Dual Prophet Ensemble (weighted)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Load and fit Dual Prophet with weighted combination\nprint(\"Fitting Dual Prophet Ensemble (weighted)...\")\n\nconfig_weighted = EnsembleConfig.from_yaml(CONFIGS_PATH / \"ensemble_dual_prophet.yaml\")\n# Modify to use weighted combination\nconfig_weighted.combine_method = \"weighted\"\nconfig_weighted.component_weights = [0.4, 0.6]  # 40% high-freq, 60% low-freq\nprint(f\"\\nConfig: {config_weighted.name}\")\nprint(f\"Combine method: {config_weighted.combine_method}\")\nprint(f\"Component weights: {config_weighted.component_weights}\")\n\nmodel_weighted = EnsembleForecaster(config_weighted)\nmodel_weighted.fit(df_train)\n\n# Display metrics\nprint(\"\\nWeighted Ensemble Metrics:\")\nprint(f\"  RMSE: {model_weighted.metrics_['rmse']:.4f}\")\nprint(f\"  MAE:  {model_weighted.metrics_['mae']:.4f}\")\nprint(f\"  R²:   {model_weighted.metrics_['r2']:.4f}\")\n\n# Predict\nfc_weighted = model_weighted.predict(df_train.tail(60 * STEPS_PER_DAY), periods=FORECAST_HORIZON)\nyhat_weighted = fc_weighted[\"yhat\"].values\n\nrmse_weighted = np.sqrt(((y_true - yhat_weighted) ** 2).mean())\nprint(f\"\\nWeighted Ensemble Forecast RMSE: {rmse_weighted:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Dual NeuralProphet Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and fit Dual NeuralProphet\n",
    "print(\"Fitting Dual NeuralProphet Ensemble...\")\n",
    "\n",
    "try:\n",
    "    config_dual_np = EnsembleConfig.from_yaml(CONFIGS_PATH / \"ensemble_dual_neuralprophet.yaml\")\n",
    "    print(f\"\\nConfig: {config_dual_np.name}\")\n",
    "    print(f\"Components: {[c.name for c in config_dual_np.components]}\")\n",
    "\n",
    "    model_dual_np = EnsembleForecaster(config_dual_np)\n",
    "    model_dual_np.fit(df_train)\n",
    "\n",
    "    # Display metrics\n",
    "    print(\"\\nDual NeuralProphet Metrics:\")\n",
    "    print(f\"  RMSE: {model_dual_np.metrics_['rmse']:.4f}\")\n",
    "    print(f\"  MAE:  {model_dual_np.metrics_['mae']:.4f}\")\n",
    "    print(f\"  R²:   {model_dual_np.metrics_['r2']:.4f}\")\n",
    "\n",
    "    # Predict\n",
    "    fc_dual_np = model_dual_np.predict(df_train.tail(60 * STEPS_PER_DAY), periods=FORECAST_HORIZON)\n",
    "    yhat_dual_np = fc_dual_np[\"yhat\"].values\n",
    "\n",
    "    rmse_dual_np = np.sqrt(((y_true - yhat_dual_np) ** 2).mean())\n",
    "    print(f\"\\nDual NeuralProphet Forecast RMSE: {rmse_dual_np:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"NeuralProphet not available or error: {e}\")\n",
    "    yhat_dual_np = None\n",
    "    rmse_dual_np = None\n",
    "    model_dual_np = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcc9c6ub7sv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dual NeuralProphet ensemble (if available)\n",
    "if model_dual_np is not None:\n",
    "    fig, ax = model_dual_np.plot(\n",
    "        df_train, window_days=7, title=\"Dual NeuralProphet Ensemble - Fit and Forecast\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Forecast Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all forecasts\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "hours = np.arange(FORECAST_HORIZON) / STEPS_PER_HOUR\n",
    "\n",
    "# Actual\n",
    "ax.plot(hours, y_true, \"-\", color=COLORS[\"actual\"], lw=2.5, label=\"Actual\")\n",
    "\n",
    "# Baseline\n",
    "ax.plot(\n",
    "    hours,\n",
    "    yhat_baseline_bias,\n",
    "    \"--\",\n",
    "    color=COLORS[\"baseline\"],\n",
    "    lw=1.5,\n",
    "    label=f\"Baseline Prophet (RMSE={rmse_baseline:.3f})\",\n",
    ")\n",
    "\n",
    "# Dual Prophet (sum)\n",
    "ax.plot(\n",
    "    hours,\n",
    "    yhat_dual_sum,\n",
    "    \"-\",\n",
    "    color=COLORS[\"dual_sum\"],\n",
    "    lw=1.5,\n",
    "    label=f\"Dual Prophet sum (RMSE={rmse_dual_sum:.3f})\",\n",
    ")\n",
    "\n",
    "# Dual Prophet (reconcile)\n",
    "ax.plot(\n",
    "    hours,\n",
    "    yhat_reconcile,\n",
    "    \"-\",\n",
    "    color=COLORS[\"dual_reconcile\"],\n",
    "    lw=1.5,\n",
    "    label=f\"Dual Prophet reconcile (RMSE={rmse_reconcile:.3f})\",\n",
    ")\n",
    "\n",
    "# Dual NeuralProphet\n",
    "if yhat_dual_np is not None:\n",
    "    ax.plot(\n",
    "        hours,\n",
    "        yhat_dual_np,\n",
    "        \"-\",\n",
    "        color=COLORS[\"dual_np\"],\n",
    "        lw=1.5,\n",
    "        label=f\"Dual NeuralProphet (RMSE={rmse_dual_np:.3f})\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Forecast Horizon (hours)\", fontsize=12)\n",
    "ax.set_ylabel(\"Temperature (°C)\", fontsize=12)\n",
    "ax.set_title(\"24h Forecast Comparison: EnsembleForecaster Models\", fontsize=14)\n",
    "ax.legend(fontsize=10, loc=\"best\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. RMSE by Lead Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE at different lead times\n",
    "def rmse_at_step(y_true, yhat, step):\n",
    "    return np.sqrt((y_true[step - 1] - yhat[step - 1]) ** 2)\n",
    "\n",
    "\n",
    "lead_times_h = [2, 4, 6, 8, 12, 16, 20, 24]\n",
    "\n",
    "results = {\"Lead Time\": lead_times_h}\n",
    "results[\"Baseline\"] = [\n",
    "    rmse_at_step(y_true, yhat_baseline_bias, h * STEPS_PER_HOUR) for h in lead_times_h\n",
    "]\n",
    "results[\"Dual Sum\"] = [\n",
    "    rmse_at_step(y_true, yhat_dual_sum, h * STEPS_PER_HOUR) for h in lead_times_h\n",
    "]\n",
    "results[\"Dual Reconcile\"] = [\n",
    "    rmse_at_step(y_true, yhat_reconcile, h * STEPS_PER_HOUR) for h in lead_times_h\n",
    "]\n",
    "\n",
    "if yhat_dual_np is not None:\n",
    "    results[\"Dual NP\"] = [\n",
    "        rmse_at_step(y_true, yhat_dual_np, h * STEPS_PER_HOUR) for h in lead_times_h\n",
    "    ]\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"RMSE at Key Lead Times:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Plot RMSE by lead time\nfig, ax = plt.subplots(figsize=(12, 5))\n\n\n# Compute rolling RMSE\ndef rolling_rmse(y_true, yhat, window=4):\n    rmse = []\n    for i in range(len(y_true)):\n        start = max(0, i - window + 1)\n        rmse.append(np.sqrt(np.mean((y_true[start : i + 1] - yhat[start : i + 1]) ** 2)))\n    return np.array(rmse)\n\n\nhours = np.arange(FORECAST_HORIZON) / STEPS_PER_HOUR\n\nax.plot(\n    hours,\n    rolling_rmse(y_true, yhat_baseline_bias),\n    \"-\",\n    color=COLORS[\"baseline\"],\n    lw=2,\n    label=\"Baseline\",\n)\nax.plot(\n    hours,\n    rolling_rmse(y_true, yhat_dual_sum),\n    \"-\",\n    color=COLORS[\"dual_sum\"],\n    lw=2,\n    label=\"Dual Sum\",\n)\nax.plot(\n    hours,\n    rolling_rmse(y_true, yhat_weighted),\n    \"-\",\n    color=COLORS[\"dual_reconcile\"],\n    lw=2,\n    label=\"Dual Weighted\",\n)\n\nif yhat_dual_np is not None:\n    ax.plot(\n        hours,\n        rolling_rmse(y_true, yhat_dual_np),\n        \"-\",\n        color=COLORS[\"dual_np\"],\n        lw=2,\n        label=\"Dual NP\",\n    )\n\nax.axhline(y=1.0, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"1°C threshold\")\nax.set_xlabel(\"Forecast Horizon (hours)\", fontsize=12)\nax.set_ylabel(\"Rolling RMSE (°C)\", fontsize=12)\nax.set_title(\"Rolling RMSE by Lead Time\", fontsize=14)\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 24)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Decomposition Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show signal decomposition\n",
    "decomposer = BandpassDecomposer(\n",
    "    freq=STEPS_PER_DAY,\n",
    "    period_pairs=[\n",
    "        (0.10, 0.70),  # Band 0: Sub-daily\n",
    "        (0.70, 4.00),  # Band 1: Daily\n",
    "        (4.00, 11.00),  # Band 2: Weekly\n",
    "        (11.00, 25.00),  # Band 3: Bi-weekly\n",
    "        (25.00, 60.00),  # Band 4: Monthly\n",
    "    ],\n",
    "    filter_type=\"savgol\",\n",
    "    savgol_polyorder=3,\n",
    "    edge_method=\"reflect\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Decompose last 14 days for visualization\n",
    "df_viz = df_train.tail(14 * STEPS_PER_DAY).copy()\n",
    "df_decomposed = decomposer.decompose(df_viz)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12), sharex=True)\n",
    "\n",
    "# Original signal\n",
    "axes[0].plot(df_decomposed[\"ds\"], df_decomposed[\"y\"], \"k\", lw=0.8)\n",
    "axes[0].set_ylabel(\"Original (°C)\")\n",
    "axes[0].set_title(\"Signal Decomposition for Ensemble Forecasting\", fontsize=14)\n",
    "\n",
    "# High-freq (bands 0-1)\n",
    "high_freq = df_decomposed[\"y_band_0\"] + df_decomposed[\"y_band_1\"]\n",
    "axes[1].plot(df_decomposed[\"ds\"], high_freq, color=\"#e74c3c\", lw=0.8)\n",
    "axes[1].set_ylabel(\"High-freq <2d (°C)\")\n",
    "axes[1].axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# Low-freq (bands 2-4)\n",
    "low_freq = df_decomposed[\"y_band_2\"] + df_decomposed[\"y_band_3\"] + df_decomposed[\"y_band_4\"]\n",
    "axes[2].plot(df_decomposed[\"ds\"], low_freq, color=\"#3498db\", lw=0.8)\n",
    "axes[2].set_ylabel(\"Low-freq >2d (°C)\")\n",
    "\n",
    "# Reconstructed\n",
    "reconstructed = high_freq + low_freq\n",
    "axes[3].plot(df_decomposed[\"ds\"], df_decomposed[\"y\"], \"k\", lw=0.8, alpha=0.5, label=\"Original\")\n",
    "axes[3].plot(df_decomposed[\"ds\"], reconstructed, \"g--\", lw=0.8, label=\"Reconstructed\")\n",
    "axes[3].set_ylabel(\"Comparison (°C)\")\n",
    "axes[3].set_xlabel(\"Date\")\n",
    "axes[3].legend(loc=\"upper right\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Variance recovery\n",
    "var_original = df_decomposed[\"y\"].var()\n",
    "var_reconstructed = reconstructed.var()\n",
    "residual = df_decomposed[\"y\"] - reconstructed\n",
    "var_residual = residual.var()\n",
    "recovery = 1 - var_residual / var_original\n",
    "\n",
    "print(f\"\\nVariance Recovery: {recovery * 100:.2f}%\")\n",
    "print(f\"High-freq variance: {high_freq.var():.4f} ({high_freq.var() / var_original * 100:.1f}%)\")\n",
    "print(f\"Low-freq variance: {low_freq.var():.4f} ({low_freq.var() / var_original * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Summary table\nprint(\"=\" * 70)\nprint(\"SUMMARY: 24h Forecast RMSE\")\nprint(\"=\" * 70)\n\nsummary = {\n    \"Model\": [\"Baseline Prophet\", \"Dual Prophet (sum)\", \"Dual Prophet (weighted)\"],\n    \"RMSE\": [rmse_baseline, rmse_dual_sum, rmse_weighted],\n    \"In-sample R²\": [\n        baseline_model.metrics_[\"r2\"],\n        model_dual_sum.metrics_[\"r2\"],\n        model_weighted.metrics_[\"r2\"],\n    ],\n}\n\nif rmse_dual_np is not None:\n    summary[\"Model\"].append(\"Dual NeuralProphet\")\n    summary[\"RMSE\"].append(rmse_dual_np)\n    summary[\"In-sample R²\"].append(model_dual_np.metrics_[\"r2\"])\n\ndf_summary = pd.DataFrame(summary)\ndf_summary[\"Improvement vs Baseline\"] = (rmse_baseline - df_summary[\"RMSE\"]) / rmse_baseline * 100\ndf_summary[\"Improvement vs Baseline\"] = df_summary[\"Improvement vs Baseline\"].apply(\n    lambda x: f\"{x:+.1f}%\"\n)\ndf_summary[\"In-sample R²\"] = df_summary[\"In-sample R²\"].apply(lambda x: f\"{x:.4f}\")\n\nprint(df_summary.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 70)\nbest_idx = pd.to_numeric(df_summary[\"RMSE\"], errors=\"coerce\").idxmin()\nbest_model = df_summary.loc[best_idx, \"Model\"]\nbest_rmse = df_summary.loc[best_idx, \"RMSE\"]\nprint(f\"BEST MODEL: {best_model} (RMSE: {best_rmse:.4f})\")\nprint(\"=\" * 70)\n\n# Show all model metrics in a nice table\nprint(\"\\n\" + \"=\" * 70)\nprint(\"IN-SAMPLE FIT METRICS (model.metrics_)\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<30} {'RMSE':>10} {'MAE':>10} {'R²':>10}\")\nprint(\"-\" * 70)\nprint(\n    f\"{'Baseline Prophet':<30} {baseline_model.metrics_['rmse']:>10.4f} {baseline_model.metrics_['mae']:>10.4f} {baseline_model.metrics_['r2']:>10.4f}\"\n)\nprint(\n    f\"{'Dual Prophet (sum)':<30} {model_dual_sum.metrics_['rmse']:>10.4f} {model_dual_sum.metrics_['mae']:>10.4f} {model_dual_sum.metrics_['r2']:>10.4f}\"\n)\nprint(\n    f\"{'Dual Prophet (weighted)':<30} {model_weighted.metrics_['rmse']:>10.4f} {model_weighted.metrics_['mae']:>10.4f} {model_weighted.metrics_['r2']:>10.4f}\"\n)\nif model_dual_np is not None:\n    print(\n        f\"{'Dual NeuralProphet':<30} {model_dual_np.metrics_['rmse']:>10.4f} {model_dual_np.metrics_['mae']:>10.4f} {model_dual_np.metrics_['r2']:>10.4f}\"\n    )\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 11. Save/Load Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate save/load functionality\n",
    "import tempfile\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    save_path = Path(tmpdir) / \"ensemble_model\"\n",
    "\n",
    "    # Save\n",
    "    model_dual_sum.save(save_path)\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "    print(f\"Contents: {list(save_path.iterdir())}\")\n",
    "\n",
    "    # Load\n",
    "    loaded_model = EnsembleForecaster.load(save_path)\n",
    "    print(\"\\nModel loaded successfully!\")\n",
    "    print(f\"Components: {[c.name for c, _ in loaded_model._components]}\")\n",
    "\n",
    "    # Verify prediction\n",
    "    fc_loaded = loaded_model.predict(df_train.tail(60 * STEPS_PER_DAY), periods=FORECAST_HORIZON)\n",
    "    print(\n",
    "        f\"\\nLoaded model prediction matches: {np.allclose(fc_loaded['yhat'].values, yhat_dual_sum)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "## 12. Conclusions\n\nKey findings from this exploration:\n\n1. **Dual Prophet with frequency decomposition** significantly outperforms baseline Prophet\n2. **Weighted combination** of frequency bands can be optimized to improve forecasts\n3. **Bias correction** is important for Prophet models (they don't weight recent observations)\n4. **NeuralProphet** may not need bias correction due to autoregressive components\n\n### New Features Demonstrated:\n\n- **`model.metrics_`**: Access in-sample fit metrics (RMSE, MAE, R²) for any fitted model\n- **`model.plot()`**: Visualize training data, fitted model, and forecast in one call\n- **`model.plot_components()`**: Panel view of each ensemble component\n- **`model.get_component_names()`**: List component names\n- **`model.high_freq_model`**: Direct access to component models via attribute\n\n### Ensemble Combination Methods:\n\n- **`sum`**: Simple sum of component forecasts (default)\n- **`weighted`**: Weighted combination with user-specified weights that sum to 1.0\n\n### Recommendations:\n- For short-term (0-4h): Dual Prophet/NeuralProphet with high-freq focus\n- For long-term (12-24h): Dual models capture weekly/monthly patterns better\n- Use `combine_method='weighted'` to tune component importance based on your data characteristics"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
